{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Classification: Airline Complaints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective: train a classifier to classify tweets as complaints or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have labeled 4,960 tweets [here](https://docs.google.com/spreadsheets/d/1rU3Gt81fwjHAcB0-a0N3rwsfquKQJjxNK838lhsCDCg/edit#gid=65146049) with binary labels of **complaint** (1) or **not a complaint** (0).\n",
    "\n",
    "The first step in classification is to represent our tweets numerically while retaining semantic information within the tweet. We do this with [Fasttext via Gensim](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "from gensim.test.utils import common_texts\n",
    "import gensim\n",
    "import keras\n",
    "import matplotlib.pylab as plt\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Embeddings with FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_train_file = '../data/tweet_sample_2M_noRT.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_train_data = LineSentence(embed_train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gensim = FastText(size=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabulary\n",
    "model_gensim.build_vocab(embed_train_data)\n",
    "\n",
    "# train the model\n",
    "model_gensim.train(embed_train_data, total_examples=model_gensim.corpus_count, epochs=model_gensim.iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model to a file and load it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving a model trained via Gensim's fastText implementation\n",
    "model_gensim.save('../models/gensim_FT_45_cbow.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=142971, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "fasttext100 = FastText.load('../models/gensim_FT_100_cbow.dat')\n",
    "print(fasttext100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the word vector for a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adriel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-2.6900437 , -0.7829389 ,  0.8260516 , -0.8935343 ,  4.5002947 ,\n",
       "       -2.0800745 ,  0.8185378 , -2.0441744 , -1.131624  , -3.5746562 ,\n",
       "       -1.1319908 , -0.55258864,  0.6499821 , -2.4114673 ,  2.1817873 ,\n",
       "        3.349079  , -0.00708565,  3.561728  , -1.7320576 ,  3.3835554 ,\n",
       "       -1.682171  , -2.6499684 , -3.4524546 , -1.6946793 ,  0.09061141,\n",
       "       -2.4246001 , -2.6531866 , -2.423885  ,  2.8988311 , -2.6459887 ,\n",
       "        3.896811  , -0.50217664,  1.1331049 ,  0.4293008 , -0.69297755,\n",
       "        1.0952688 ,  3.4780877 , -1.5056956 ,  3.2781224 , -0.8678973 ,\n",
       "        0.8762853 ,  2.0280821 ,  0.40427354, -2.909961  , -0.37729537,\n",
       "        2.6488566 ,  2.0457501 ,  0.67427605,  0.1820736 , -2.3562267 ,\n",
       "       -0.6233044 , -0.4258164 , -2.7493412 , -0.05465397,  1.4000791 ,\n",
       "        0.8776595 ,  2.2817457 ,  0.24748203,  0.09730937, -2.567825  ,\n",
       "       -1.403272  , -0.7174882 , -4.0232043 ,  1.8875126 ,  1.2348607 ,\n",
       "        0.69388485, -1.2996528 ,  0.40212575, -1.057632  , -2.219325  ,\n",
       "        2.2659242 , -2.003948  , -0.11093061, -0.13382712,  1.86079   ,\n",
       "        0.6983327 ,  1.0600638 ,  0.98915225, -2.3732762 ,  2.5885818 ,\n",
       "       -1.1556091 , -0.06924234, -2.0606034 , -1.4539673 ,  1.1963664 ,\n",
       "       -4.5320587 , -2.181204  , -2.3700137 ,  0.82906264, -0.130299  ,\n",
       "       -2.7018678 ,  0.15886174,  1.2682315 ,  1.9302173 , -3.6516614 ,\n",
       "       -1.0489181 , -1.845395  ,  3.4667513 , -0.5901399 , -3.523942  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext100[\"delta\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"../data/Marketing Research Labeled Tweets_ - tweet_sample_5k_FULL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>two airports, one green grass and one sandy co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>bismillahi majreha wa mursaha inna robbi la gh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@americanair i understand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@jae_nita @delta i'll make it up to you come t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>@jetblue why are your employees so rude today ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>@usairways dg: “it is a pic of a woman…&amp;amp; s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>@icelandair awesome thanks for these recommend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>@emirates good idea!;)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>@americanair voila! careers site feedback page...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>@airlineflyer @baltiausa should be a relief fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                         tweet_text\n",
       "0      0  two airports, one green grass and one sandy co...\n",
       "1      0  bismillahi majreha wa mursaha inna robbi la gh...\n",
       "2      0                          @americanair i understand\n",
       "3      0  @jae_nita @delta i'll make it up to you come t...\n",
       "4      1  @jetblue why are your employees so rude today ...\n",
       "5      0  @usairways dg: “it is a pic of a woman…&amp; s...\n",
       "6      0  @icelandair awesome thanks for these recommend...\n",
       "7      0                             @emirates good idea!;)\n",
       "8      0  @americanair voila! careers site feedback page...\n",
       "9      0  @airlineflyer @baltiausa should be a relief fo..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_text\n",
       "label            \n",
       "0            3621\n",
       "1            1338"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.groupby(\"label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>two airports, one green grass and one sandy co...</td>\n",
       "      <td>[two, airports, one, green, grass, and, one, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>bismillahi majreha wa mursaha inna robbi la gh...</td>\n",
       "      <td>[bismillahi, majreha, wa, mursaha, inna, robbi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@americanair i understand</td>\n",
       "      <td>[americanair, i, understand]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@jae_nita @delta i'll make it up to you come t...</td>\n",
       "      <td>[jae_nita, delta, i, ll, make, it, up, to, you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>@jetblue why are your employees so rude today ...</td>\n",
       "      <td>[jetblue, why, are, your, employees, so, rude,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                         tweet_text  \\\n",
       "0      0  two airports, one green grass and one sandy co...   \n",
       "1      0  bismillahi majreha wa mursaha inna robbi la gh...   \n",
       "2      0                          @americanair i understand   \n",
       "3      0  @jae_nita @delta i'll make it up to you come t...   \n",
       "4      1  @jetblue why are your employees so rude today ...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [two, airports, one, green, grass, and, one, s...  \n",
       "1  [bismillahi, majreha, wa, mursaha, inna, robbi...  \n",
       "2                       [americanair, i, understand]  \n",
       "3  [jae_nita, delta, i, ll, make, it, up, to, you...  \n",
       "4  [jetblue, why, are, your, employees, so, rude,...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tweets[\"tokens\"] = tweets[\"tweet_text\"].apply(tokenizer.tokenize)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78062 words total, with a vocabulary size of 14514\n",
      "Max sentence length is 33\n"
     ]
    }
   ],
   "source": [
    "all_words = [word for tokens in tweets[\"tokens\"] for word in tokens]\n",
    "sentence_lengths = [len(tokens) for tokens in tweets[\"tokens\"]]\n",
    "VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(sentence_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "MAX_SEQUENCE_LENGTH = 33\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "\n",
    "VALIDATION_SPLIT=.2\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(tweets[\"tweet_text\"].tolist())\n",
    "sequences = tokenizer.texts_to_sequences(tweets[\"tweet_text\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14966 unique tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adriel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/adriel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14967, 100)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "cnn_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(tweets[\"label\"]))\n",
    "\n",
    "indices = np.arange(cnn_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "cnn_data = cnn_data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * cnn_data.shape[0])\n",
    "\n",
    "embedding_weights = np.zeros((len(word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in word_index.items():\n",
    "    embedding_weights[index,:] = fasttext100[word] if word in fasttext100 else np.random.rand(EMBEDDING_DIM)\n",
    "embedding_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Flatten, Dropout, Add, Concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.models import Model\n",
    "\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index, trainable=False, extra_conv=True):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=trainable)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
    "    pool = MaxPooling1D(pool_size=3)(conv)\n",
    "\n",
    "    if extra_conv==True:\n",
    "        x = Dropout(0.5)(l_merge)  \n",
    "    else:\n",
    "        # Original Yoon Kim model\n",
    "        x = Dropout(0.5)(pool)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "\n",
    "    preds = Dense(labels_index, activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = cnn_data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = cnn_data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "model = ConvNet(embedding_weights, MAX_SEQUENCE_LENGTH, len(word_index)+1, EMBEDDING_DIM, \n",
    "                len(list(tweets[\"label\"].unique())), False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First attempt with embedding vector length 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=30, batch_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Attempt with Embedding Vector Length 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3968 samples, validate on 991 samples\n",
      "Epoch 1/30\n",
      "3968/3968 [==============================] - 2s 387us/step - loss: 0.0058 - acc: 0.9972 - val_loss: 1.3275 - val_acc: 0.8194\n",
      "Epoch 2/30\n",
      "3968/3968 [==============================] - 2s 390us/step - loss: 0.0045 - acc: 0.9980 - val_loss: 1.3158 - val_acc: 0.8194\n",
      "Epoch 3/30\n",
      "3968/3968 [==============================] - 2s 386us/step - loss: 0.0030 - acc: 0.9990 - val_loss: 1.2919 - val_acc: 0.8194\n",
      "Epoch 4/30\n",
      "3968/3968 [==============================] - 2s 394us/step - loss: 0.0057 - acc: 0.9982 - val_loss: 1.3090 - val_acc: 0.8184\n",
      "Epoch 5/30\n",
      "3968/3968 [==============================] - 2s 399us/step - loss: 0.0038 - acc: 0.9982 - val_loss: 1.2737 - val_acc: 0.8194\n",
      "Epoch 6/30\n",
      "3968/3968 [==============================] - 2s 387us/step - loss: 0.0126 - acc: 0.9955 - val_loss: 1.2493 - val_acc: 0.8194\n",
      "Epoch 7/30\n",
      "3968/3968 [==============================] - 2s 400us/step - loss: 0.0058 - acc: 0.9970 - val_loss: 1.2458 - val_acc: 0.8194\n",
      "Epoch 8/30\n",
      "3968/3968 [==============================] - 2s 398us/step - loss: 0.0041 - acc: 0.9985 - val_loss: 1.2858 - val_acc: 0.8264\n",
      "Epoch 9/30\n",
      "3968/3968 [==============================] - 2s 408us/step - loss: 0.0061 - acc: 0.9982 - val_loss: 1.3190 - val_acc: 0.8143\n",
      "Epoch 10/30\n",
      "3968/3968 [==============================] - 2s 394us/step - loss: 0.0064 - acc: 0.9977 - val_loss: 1.3197 - val_acc: 0.8103\n",
      "Epoch 11/30\n",
      "3968/3968 [==============================] - 2s 406us/step - loss: 0.0025 - acc: 0.9992 - val_loss: 1.3152 - val_acc: 0.8204\n",
      "Epoch 12/30\n",
      "3968/3968 [==============================] - 2s 415us/step - loss: 0.0046 - acc: 0.9990 - val_loss: 1.3435 - val_acc: 0.8224\n",
      "Epoch 13/30\n",
      "3968/3968 [==============================] - 2s 405us/step - loss: 0.0073 - acc: 0.9980 - val_loss: 1.3090 - val_acc: 0.8153\n",
      "Epoch 14/30\n",
      "3968/3968 [==============================] - 2s 405us/step - loss: 0.0025 - acc: 0.9995 - val_loss: 1.3871 - val_acc: 0.8143\n",
      "Epoch 15/30\n",
      "3968/3968 [==============================] - 2s 398us/step - loss: 0.0054 - acc: 0.9985 - val_loss: 1.3795 - val_acc: 0.8163\n",
      "Epoch 16/30\n",
      "3968/3968 [==============================] - 2s 409us/step - loss: 0.0057 - acc: 0.9982 - val_loss: 1.3458 - val_acc: 0.8103\n",
      "Epoch 17/30\n",
      "3968/3968 [==============================] - 2s 412us/step - loss: 0.0059 - acc: 0.9987 - val_loss: 1.3513 - val_acc: 0.8133\n",
      "Epoch 18/30\n",
      "3968/3968 [==============================] - 2s 412us/step - loss: 0.0059 - acc: 0.9977 - val_loss: 1.4013 - val_acc: 0.8093\n",
      "Epoch 19/30\n",
      "3968/3968 [==============================] - 2s 414us/step - loss: 0.0030 - acc: 0.9987 - val_loss: 1.4099 - val_acc: 0.8093\n",
      "Epoch 20/30\n",
      "3968/3968 [==============================] - 2s 424us/step - loss: 0.0062 - acc: 0.9990 - val_loss: 1.4015 - val_acc: 0.8123\n",
      "Epoch 21/30\n",
      "3968/3968 [==============================] - 2s 419us/step - loss: 0.0053 - acc: 0.9985 - val_loss: 1.4013 - val_acc: 0.8194\n",
      "Epoch 22/30\n",
      "3968/3968 [==============================] - 2s 433us/step - loss: 0.0045 - acc: 0.9977 - val_loss: 1.3951 - val_acc: 0.8224\n",
      "Epoch 23/30\n",
      "3968/3968 [==============================] - 2s 428us/step - loss: 0.0073 - acc: 0.9987 - val_loss: 1.3875 - val_acc: 0.8184\n",
      "Epoch 24/30\n",
      "3968/3968 [==============================] - 2s 432us/step - loss: 0.0070 - acc: 0.9977 - val_loss: 1.3837 - val_acc: 0.8204\n",
      "Epoch 25/30\n",
      "3968/3968 [==============================] - 2s 426us/step - loss: 0.0045 - acc: 0.9987 - val_loss: 1.3492 - val_acc: 0.8133\n",
      "Epoch 26/30\n",
      "3968/3968 [==============================] - 2s 420us/step - loss: 0.0071 - acc: 0.9977 - val_loss: 1.3195 - val_acc: 0.8184\n",
      "Epoch 27/30\n",
      "3968/3968 [==============================] - 2s 422us/step - loss: 0.0041 - acc: 0.9980 - val_loss: 1.3044 - val_acc: 0.8274\n",
      "Epoch 28/30\n",
      "3968/3968 [==============================] - 2s 431us/step - loss: 0.0075 - acc: 0.9985 - val_loss: 1.3701 - val_acc: 0.8194\n",
      "Epoch 29/30\n",
      "3968/3968 [==============================] - 2s 444us/step - loss: 0.0066 - acc: 0.9982 - val_loss: 1.3563 - val_acc: 0.8204\n",
      "Epoch 30/30\n",
      "3968/3968 [==============================] - 2s 433us/step - loss: 0.0031 - acc: 0.9995 - val_loss: 1.3238 - val_acc: 0.8214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2a9975c0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=30, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccuracyHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.acc = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.acc.append(logs.get('acc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AccuracyHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AccuracyHistory' object has no attribute 'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-6b16d4610e32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AccuracyHistory' object has no attribute 'acc'"
     ]
    }
   ],
   "source": [
    "plt.plot(range(1,11), history.acc)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Attempt with Embedding Vector Length 25 and a Larger Embedding Training Corpus (2mil instead of 1.5 mil tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=30, batch_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth Attempt with Embedding Vector Length 45 and a Larger Embedding Training Corpus (2mil instead of 1.5 mil tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=30, batch_size=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Add Labels\n",
    "We will use Fastexts supervised learning functionality to classify our tweets. The format that fasttext uses for training a supervised classifier is as follows:\n",
    "\n",
    "    __label__complaint @jetblue why are your employees so rude today at dallas-fort worth? tons of attitude on simple questions. #notimpressed\n",
    "    __label__notcomplaint @icelandair awesome thanks for these recommendations! @sarahamil and i are very excited!!\n",
    "\n",
    "The Fasttext model trains on a txt file that contains sentences with one sentence per line and each sentence preceded by a label prefixed by ``__label__``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **Bag of Tricks for Efficient Text Classification** https://arxiv.org/pdf/1607.01759.pdf \n",
    "\n",
    "2) **Convolutional Neural Networks for Sentence Classification** https://arxiv.org/abs/1408.5882\n",
    "\n",
    "3) **How to solve 90% of NLP problems: a step-by-step guide** https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e \n",
    "\n",
    "4) **Fasttext Classification** https://github.com/facebookresearch/fastText#text-classification\n",
    "\n",
    "5) **Gensim Wrapper for Fasttext** https://radimrehurek.com/gensim/models/fasttext.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

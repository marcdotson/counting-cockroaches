---
title: "Case Study (McKenna)"
author: "McKenna Weech"
date: "3/10/2020"
output: html_document
---


Firms employ a variety of passive listening platforms (e.g., customer comment cards, help lines, social media) that elicit a variety of complaints. Since these complaints are self-selected, it isnâ€™t obvious if complaints are widespread or genuinely indicative of systemic service failures. In other words, how can can a firm know when complaints can be ignored and when they need to addressed? This issue is related to a larger class of problems where the goal is to make inference about the size of a population from a non-random sample (e.g., the German tank problem). In this paper, we develop a model that uses a variety of observed customer complaints on Twitter to make inference about the severity of service failures.

## Describe the model conceptually

It has been said that if you observe a cockroach on your floor, there are likely thousands inside the walls of your home. Following this analogy, our hope is to count the cockroaches we can see and calculate how many are hiding in the walls.

For the first portion of this case, we will be making a model around simulated data. Each observation will represent a single tweet, and we will be scoring them for satisfaction. We will use a multiple hierarchal linear model to predict satisfaction from the other known variables. Once we have the model predicting well on our simulated data, we will fit it to our real data, and from there use our real data satisfaction scores to measure service or product failure severity.

Along with making a model, we will also be using poststratification to correct the weights in our simulated sample and real data to more correctly represent the population. This will be explained in greater detail as we postratify in this first iteration of the model, so we can explain and see the benefit of a full multiple regression with postratification model (MRP).

To begin, we will start by generating our simulated data. It will run through an initial simple linear model and postratification.


# Iteration 1: Simulation Data and Simple Model

This initial block of code, we will just be loading in the needed packages and settings to make our sample and model code run.

```{r message = FALSE}
library(tidyverse) #Load packages
library(rstan)
library(rstanarm)
library(ggplot2)
library(bayesplot)
theme_set(bayesplot::theme_default())
library(dplyr)
library(tidyr)
library(tidybayes)

options(mc.cores = parallel::detectCores()) # Set Stan to use all availible cores
rstan_options(auto_write = TRUE) # Don't recompile Stan code that hasn't changed
```

For this first iteration of modeling, we will be generating our own data. We will generate a population, and sample from it in order to build an MRP model. 

We will be modeling our populations `service_failure`. This will eventually translate into measuring service severity when we apply our model to our real data. We will be looking at common characteristics like `gender`, `ethnicity`, `income`, `age`, and `state`. 

The following code will make a function `simulate_mrp_data` that will help us make our simulated data, and sample from it in such a way that we can use and see the value in postratification.

```{r}
simulate_mrp_data <- function(n) {
  J <- c(2, 3, 7, 3, 50) # male or not, eth, age, income level, state
  poststrat <- as.data.frame(array(NA, c(prod(J), length(J)+1))) # Columns of post-strat matrix, plus one for size
  colnames(poststrat) <- c("male", "eth", "age","income", "state",'N')
  count <- 0
  for (i1 in 1:J[1]){
    for (i2 in 1:J[2]){
      for (i3 in 1:J[3]){
        for (i4 in 1:J[4]){
          for (i5 in 1:J[5]){
              count <- count + 1
              # Fill them in so we know what category we are referring to
              poststrat[count, 1:5] <- c(i1-1, i2, i3, i4, i5)
          }
        }
      }
    }
  }
  # Proportion in each sample in the population
  p_male <- c(0.52, 0.48)
  p_eth <- c(0.5, 0.2, 0.3)
  p_age <- c(0.2,.1,0.2,0.2, 0.10, 0.1, 0.1)
  p_income<-c(.50,.35,.15)
  p_state_tmp<-runif(50,10,20)
  p_state<-p_state_tmp/sum(p_state_tmp)
  poststrat$N<-0
  for (j in 1:prod(J)){
    poststrat$N[j] <- round(250e6 * p_male[poststrat[j,1]+1] * p_eth[poststrat[j,2]] *
      p_age[poststrat[j,3]]*p_income[poststrat[j,4]]*p_state[poststrat[j,5]]) #Adjust the N to be the number observed in each category in each group
  }

  # Now let's adjust for the probability of response
  p_response_baseline <- 0.01
  p_response_male <- c(2, 0.8) / 2.8
  p_response_eth <- c(1, 1.2, 2.5) / 4.7
  p_response_age <- c(1, 0.4, 1, 1.5,  3, 5, 7) / 18.9
  p_response_inc <- c(1, 0.9, 0.8) / 2.7
  p_response_state <- rbeta(50, 1, 1)
  p_response_state <- p_response_state / sum(p_response_state)
  p_response <- rep(NA, prod(J))
  for (j in 1:prod(J)) {
    p_response[j] <-
      p_response_baseline * p_response_male[poststrat[j, 1] + 1] *
      p_response_eth[poststrat[j, 2]] * p_response_age[poststrat[j, 3]] *
      p_response_inc[poststrat[j, 4]] * p_response_state[poststrat[j, 5]]
  }
  people <- sample(prod(J), n, replace = TRUE, prob = poststrat$N * p_response)

  ## For respondent i, people[i] is that person's poststrat cell,
  ## some number between 1 and 32
  n_cell <- rep(NA, prod(J))
  for (j in 1:prod(J)) {
    n_cell[j] <- sum(people == j)
  }

  coef_male <- c(0,-0.3)
  coef_eth <- c(0, 0.6, 0.9)
  coef_age <- c(0,-0.2,-0.3, 0.4, 0.5, 0.7, 0.8, 0.9)
  coef_income <- c(0,-0.2, 0.6)
  coef_state <- c(0, round(rnorm(49, 0, 1), 1))
  coef_age_male <- t(cbind(c(0, .1, .23, .3, .43, .5, .6),
                           c(0, -.1, -.23, -.5, -.43, -.5, -.6)))
  true_popn <- data.frame(poststrat[, 1:5], service_failure = rep(NA, prod(J)))
  for (j in 1:prod(J)) {
    true_popn$satisfaction[j] <- plogis(
      coef_male[poststrat[j, 1] + 1] +
        coef_eth[poststrat[j, 2]] + coef_age[poststrat[j, 3]] +
        coef_income[poststrat[j, 4]] + coef_state[poststrat[j, 5]] +
        coef_age_male[poststrat[j, 1] + 1, poststrat[j, 3]]
      )
  }

  #male or not, eth, age, income level, state, city
  y <- rbinom(n, 1, true_popn$satisfaction[people])
  male <- poststrat[people, 1]
  eth <- poststrat[people, 2]
  age <- poststrat[people, 3]
  income <- poststrat[people, 4]
  state <- poststrat[people, 5]

  sample <- data.frame(service_failure = y,
                       male, age, eth, income, state,
                       id = 1:length(people))

  #Make all numeric:
  for (i in 1:ncol(poststrat)) {
    poststrat[, i] <- as.numeric(poststrat[, i])
  }
  for (i in 1:ncol(true_popn)) {
    true_popn[, i] <- as.numeric(true_popn[, i])
  }
  for (i in 1:ncol(sample)) {
    sample[, i] <- as.numeric(sample[, i])
  }
  list(
    sample = sample,
    poststrat = poststrat,
    true_popn = true_popn
  )
}
```

```{r}

mrp_sim <- simulate_mrp_data(n=1200)

sample <- mrp_sim[["sample"]]
poststrat <- mrp_sim[["poststrat"]]
true_popn <- mrp_sim[["true_popn"]]

head(sample)

```
```{r}
stan_data <- list( 
  N = nrow(sample), 
  service_failure = sample$service_failure,
  male = sample$male, 
  age = sample$age,
  eth = sample$eth, 
  income = sample$income, 
  state = sample$state)
```

```{r}
fit <- stan(
  file = here::here("Code", "mrp", "logit_simple.stan"),
  data = stan_data,
  seed = 42
)

```
```{r, eval = FALSE}
fit %>%
  mcmc_trace(
    pars = c('alpha','beta1', 'beta2', 'beta3', 'beta4'),
    n_warmup = 500,
    facet_args = list(nrow = 2, labeller = label_parsed)
  )

print(fit, pars = c('alpha','beta1', 'beta2', 'beta3', 'beta4'))
```

We now have need for a logistic regresson model in order to predict a binary outcome for 'service_failure' so we will make a hierarchal logit model

Trying to make the simple model work with the data from the vignette. It seems to not be working but the error code is not clear on why it's not working. 
```{r}
 matrix <- sample %>% 
  select(male, age, eth, income, state)

x <- as.matrix(matrix)
```


```{r}
stan_data <- list(
  N = 1200,                                       # Number of observations.
  K = 6300,                                       # Number of groups.
  g = sample(6300, 1200, replace = TRUE),         # Vector of group assignments
  
  male = sample$male,                 
  eth = sample$eth,      
  age = sample$age,        
  income = sample$income,
  state = sample$state,
  service_failure = sample$service_failure
)

```

```{r}
data_list <- list(
  D <- 5,                                    # Number of variables. 
  N <- 1200,                                 # Number of observations.
  L <- 6300,                                 # Number of groups. 
  y <- sample$service_failure,               # Outcome variables. 
  ll <- sample(6300, 1200, replace = TRUE),  # Group assignment. 
  x <- as.matrix(matrix)                     # Vector of predictors. 
)
```

```{r, eval = FALSE}
fit <- stan(
  file = here::here("Code", "mrp", "logit_hier.stan"),
  data = data_list,
  seed = 42
)
```


### Postratification 
Note: for now I have set these chuncks to eval = false because we don
t have a poststrat_prob yet so it freaks out 

For the postratification portion we take the estimate that we get from the model times poststrat$/N / sum(postsrat$N) 

```{ echo=FALSE}
poststrat_prob <- posterior_prob %*% poststrat$N / sum(poststrat$N)
model_popn_pref <- c(mean = mean(poststrat_prob), sd = sd(poststrat_prob))
round(model_popn_pref, 3)
```

Because we are using simulated data we can see how our poststratified predictions compare to the true population 

```{ echo=FALSE}
sample_popn_pref <- mean(sample$satisfaction)
round(sample_popn_pref, 3)

compare2 <- compare2 +
  geom_hline(yintercept = model_popn_pref[1], colour = '#2ca25f', size = 1) +
  geom_text(aes(x = 5.2, y = model_popn_pref[1] + .025), label = "MRP", colour = '#2ca25f')
bayesplot_grid(compare, compare2,
               grid_args = list(nrow = 1, widths = c(8, 1)))
```


---
title: "Classifier Construction"
output: github_document
---

# Overview
These are my annotations of the "classifier-construction.R" script written by Adriel.

This file is extremely important because it is where Adriel built the "frankenstein" model.

Here are two important models built by this script:
model50-tweet_sample_2M_noRT.dat
xgb.modelPC4-xgb_version.dat

# Libraries
```{r}
library(tidyverse)
library(fastrtext)
library(magrittr) # for assignment operator %<>%  Dallin: I will try to remove the use of %<>% to comply with the tidyverese stylguide.
library(plotly)
require(randomForest)
library(Matrix)
source("05_define_clustering_functions.R")
source("06_define_prediction_functions.R")
# source("../complaint-prediction/R/logit_classifier_func.R") This file does not exist

library(xgboost)
library(caret)
library(car)

set.seed(123)
```

##### Look at working directory
```{r}
getwd()
```



# Train and Load the fastText Model
Unsupervised fastText models produce word vectors. In other words, the create a mathmatical representation of the word. These word vectors will be used later to 
```{r}
# Fasttext train ----------------------------------------------------------
# train unsupervised fasttext model to learn tweet semantics and word vector representations

# file to read tweets from. Should be a txt file with one tweet per line
file_txt <- here::here("Temporary", "tweet_sample_1hM_noRT.txt")

# new file to write the trained model to
file_model50  <- here::here("Temporary", "model50-tweet_sample_2M_noRT.dat")

# train the unsupervised fasttext model
# Note: I've commented out the training of this model because it has already been trained and the output has been saved to the Temporary folder.
# fastrtext::execute(commands = c("cbow", "-input", file_txt, "-output", file_model50, "-verbose", 1, "-dim", 50))

# load the model object we just trained
fasttext_model <- load_model(file_model50)
```

# FastText Model Evaluation
This block looks at a handfull of words and creates a visualization to see how well the fasttext model predicts words similar to those provodied (the words defined as nn_words).
```{r}
# Examine the nearest neighbors returned by our trained model. 
# First vector element is the word to look up NN, and the second is a string used to filter the results

nn_words  <- list(c("mad", "mad"),
                  c("sad","sad"),
                  c("upset", "upset"),
                  c("late", "ate"),
                  c("delay", "del"),
                  c("cancelled", "cancel"),
                  c("snow", "snow"),
                  c("rain", "rain"),
                  c("weather", "weather"))

map(nn_words, function(x) model_eval_plot_wrap(x[1], fasttext_model, filter_str = x[2], n = 20))
```

# Create Average word Vectors
Here, our training dataset is brought in. With the data loaded, we featureize the tweets and create average tweet vectors. The features of the tweets and average vectors are stored inside a list-column to help organize our data well.

```{r}
# Predict and average word vectors for tweets -----------------------------

# read in labeled traing data
# train <- read_csv("data/Marketing Research Labeled Tweets All 12-05-18.csv") This was Adriel's code, but I have the data stored somewhere else.

# I believe the file I'm linking to will be similar to the one that Adriel linked imported above (commented out).
train <- read_csv(here::here("Temporary", "Marketing Research Labeled Tweets_ - tweet_sample_5k_Ky-Ch-Ad.csv"))
# Update: I tried this and I think it causes problems in the code block under "# create a list column..." (currently line 69)

# Add extracted text features to the dataset ------------------------------

train <- train %>% 
  mutate(tweetFeatures = map(tweet_text, TweetFeatures)) 


# create a list column that contains all of features output by the fasttext model
train <- train %>%
  mutate(avg_vec_50 = map(tweet_text, VectorizeTweet, model = fasttext_model))
```

### Analyzing the Outputs of What We Did Above
The two sections below are the outputs of the code we ran above. for the example tweet first tweet, shown below.
```{r}
train$tweet_text[[1]]
```


##### Resulting Tweet Features
This is the output of the tweetFeatures() function. It just makes a small table of data explaining features of each tweet, such as the number of spaces, the number of exclamation points, the number of characters, etc. The data table shown below is the output for just one tweet. This small data table is then stored inside a list-column of the train data.
```{r}
train$tweetFeatures[[1]]
```


##### Resulting Tweet Vectors
Similar to the output above, this is the output of the VectorizeTweet() function for just one tweet. If you are familiar with word vectors, you will notice that this small data table looks similar to a single word vector. This vector, however, is a compilation of every word vector from each word in a given tweet. With the example tweet above, the VectorizeTweet() function finds the word vector for the words "two," "airports," "one," "greeen," etc. The VectorizeTweet() function takes the word vector for each word in the tweet and averages each column. In theory, this should provide a mathematical summary for the entire tweet.
```{r}
train$avg_vec_50[[1]]
```




# PCA Analysis on the fastText Tweet Vectors

### Train PCA Model
This block of code trains the PCA model exclusively on the avg_vec_50 variable (the average tweet vector for each tweet).
```{r}
# select the average vector column and perform a PCA
train.pr <- train %>%
  select(avg_vec_50) %>% 
  unnest(avg_vec_50) %>% 
  as.matrix() %>% 
  prcomp()
```

### Evaluate the PCA Model with 3D Graph
Here, we create a dataframe with the PCA output for each tweet, paired with the tweet labels and tweet text. The data is then plotted with the three PCA variables on the axes, and color mapped to complaint type. With this visualization, we can see that the PCA model groups complaints and non-complaints well.
```{r}
# create a dataframe with the pca output to examine the first 3 components

pca_evaluation <- as_data_frame(train.pr$x)
pca_evaluation$label <- factor(train$complaint_label)
pca_evaluation$tweet_text <- train$tweet_text

# evalute the first three components with a 3d plot

plot_ly(pca_evaluation, x = ~PC1, y = ~PC2, z = ~PC3, color = ~label, colors = c('#BF382A', '#0C4B8E'),
        hoverinfo = 'text', text = ~tweet_text) %>% 
  add_markers() %>% 
  layout(scene = list(xaxis = list(title = 'PC1'),
                      yaxis = list(title = 'PC2'),
                      zaxis = list(title = 'PC3')))

# looks good to me. The complaints seperate well.
```

### Scree Plot of Principal Components
```{r}
# make a scree plot of the components
screeplot(train.pr, type = "line", npcs = 20)

# the first component must be the amount of information about a word learned by the fasttext model
# let's go with 15 components with this model.


```


# Project Word Vectors onto PCA
Here, we run the VectorizeTweet (PCA class) function on each of the tweet vectors. The VectorizeTweet PCA output summarizes the tweet vector in four principal components.
```{r}
train <- train %>% 
  mutate(avg_vec_50_PCA = map(avg_vec_50, VectorizeTweet, model = train.pr, component = "PC4"))
```


```{r}

save(train.pr, file = here:here("Output", "prcomp4.dat"))

save(train, file = here:here("Output", "train_data.dat"))

rm(list = ls())

load(here:here("Output", "train_data.dat"))

trainIndex <- createDataPartition(train$complaint_label, p = .8, 
                                  list = FALSE, 
                                  times = 1)


test_data <- train[-trainIndex,]
test_labels <- test_data %>% select(complaint_label) %>% transmute(label = complaint_label)
train_data <- train[trainIndex,]
train_labels <- train_data %>% select(complaint_label) %>% transmute(label = complaint_label)
train_data_vec <- train_data %>%  select(tweetFeatures, avg_vec_50, avg_vec_50_PCA) %>% unnest %>% mutate_all(as.numeric)
test_data_vec <- test_data %>%  select(tweetFeatures, avg_vec_50, avg_vec_50_PCA) %>% unnest %>% mutate_all(as.numeric)

dtrain <- xgb.DMatrix(sparse.model.matrix(~.-1, train_data_vec), label = as.numeric(train_labels$label))
dtest <- xgb.DMatrix(sparse.model.matrix(~.-1, test_data_vec), label = as.numeric(test_labels$label))
dtest1 <- xgb.DMatrix(sparse.model.matrix(~.-1, test_data_vec))
watchlist <- list(train = dtrain, eval = dtest)


param <- list(max_depth = 2, eta = 1, silent = 1, nthread = 2, 
              objective = logregobj, eval_metric = evalerror)

bst <- xgboost(data = sparse.model.matrix(~.-1, train_data_vec), label = as.numeric(train_labels$label), max.depth = 4,
               eta = 0.2, nthread = 2, nrounds = 100, objective = logregobj, eval_metric = evalerror)

test_data$prediction <- predict(bst, dtest1)

predict(bst, dtest1) %>% map_chr(function(x) ifelse(x > 0, "complaint", "non-complaint"))
  

test_data %<>%
  mutate(predict_label = case_when(prediction > 0 ~ "Complaint",
                                   TRUE ~ "Non-complaint"),
         complaint_label = case_when(complaint_label == 1 ~ "Complaint",
                                   TRUE ~ "Non-complaint")) %>% 
  mutate(accuracy = predict_label == complaint_label)

confusionMatrix(factor(test_data$complaint_label), factor(test_data$predict_label))

save(bst, file = here:here("Output", "xgb.modelPC4.dat"))
xgb.save(bst, here:here("Output", "xgb.modelPC4-xgb_version.dat"))
save(test_data, file = here::here("Output", "test_data.dat"))


```


---
title: "Classifier Construction"
output: github_document
---

# Overview
These are my annotations of the "classifier-construction.R" script written by Adriel.

This file is extremely important because it is where Adriel built the "frankenstein" model.

Here are two important models built by this script:
model50-tweet_sample_2M_noRT.dat
xgb.modelPC4-xgb_version.dat

# Libraries
```{r}
library(tidyverse)
library(fastrtext)
library(magrittr) # for assignment operator %<>%  Dallin: I will try to remove the use of %<>% to comply with the tidyverese stylguide.
library(plotly)
require(randomForest)
library(Matrix)
source("05_define_clustering_functions.R")
source("06_define_prediction_functions.R")
# source("../complaint-prediction/R/logit_classifier_func.R") This file does not exist

library(xgboost)
library(caret)
library(car)

set.seed(123)
```

##### Look at working directory
```{r}
getwd()
```



# Train and Load the fastText Model
Unsupervised fastText models produce word vectors. In other words, the create a mathmatical representation of the word. 
```{r}
# Fasttext train ----------------------------------------------------------
# train unsupervised fasttext model to learn tweet semantics and word vector representations

# file to read tweets from. Should be a txt file with one tweet per line
# file_txt <- "../../Temporary/tweet_sample_2M_noRT.txt"
file_txt <- "../../Temporary/tweet_sample_1hM_noRT.txt"

# new file to write the trained model to
file_model50  <- "../../Temporary/model50-tweet_sample_2M_noRT.dat"

# train the unsupervised fasttext model
# I've commented out the training of this model because it has already been trained and the output has been saved to the Temporary folder.

# fastrtext::execute(commands = c("cbow", "-input", file_txt, "-output", file_model50, "-verbose", 1, "-dim", 50))

# load the model object we just trained
fasttext_model <- load_model(file_model50)
```

# FastText Model Evaluation
This block looks at a handfull of words and creates a visualization to see how well the fasttext model predicts words similar to those provodied (the words defined as nn_words).
```{r}
# Examine the nearest neighbors returned by our trained model. 
# First vector element is the word to look up NN, and the second is a string used to filter the results

nn_words  <- list(c("mad", "mad"),
                  c("sad","sad"),
                  c("upset", "upset"),
                  c("late", "ate"),
                  c("delay", "del"),
                  c("cancelled", "cancel"),
                  c("snow", "snow"),
                  c("rain", "rain"),
                  c("weather", "weather"))

map(nn_words, function(x) model_eval_plot_wrap(x[1], fasttext_model, filter_str = x[2], n = 20))
```

# Create Average word Vectors
Here, our training dataset is brought in. With the data loaded, we featureize the tweets and create average tweet vectors. The features of the tweets and average vectors are stored inside a list-column to help organize our data well.

```{r}
# Predict and average word vectors for tweets -----------------------------

# read in labeled traing data
# train <- read_csv("data/Marketing Research Labeled Tweets All 12-05-18.csv") This was Adriel's code, but I have the data stored somewhere else.

# I believe the file I'm linking to will be similar to the one that Adriel linked imported above (commented out).
train <- read_csv("../../Temporary/train_test_data/Marketing Research Labeled Tweets_ - tweet_sample_5k_Ky-Ch-Ad.csv") 
# Update: I tried this and I think it causes problems in the code block under "# create a list column..." (currently line 69)

# Add extracted text features to the dataset ------------------------------

train <- train %>% 
  mutate(tweetFeatures = map(tweet_text, TweetFeatures)) 


# create a list column that contains all of features output by the fasttext model
train <- train %>%
  mutate(avg_vec_50 = map(tweet_text, VectorizeTweet, model = fasttext_model))

# Dallin: It looks like there is a problem with the map() or VectorizeTweet function above. 
# It doesn't seem to work well with mutate(). 
# I don't understand how Adriel got this to work. ¯\_(ツ)_/¯
# The little chunk below is myself troubleshooting an error.

# I tried troubleshooting the problem with this code below, but it didn't work well
# train <- train %>%
#      mutate(avg_vec_50 = map(tweet_text, avg_word_vec(text = ., model = fasttext_model)))

# I think I've fixed the problem by going into predict_func.R and removing the tbl_df() function and replacing it
# with the as_tibble() function. It looks like the documentation is saying that the tbl_df() function has been removed
# or something?
```

### Resulting Tweet Vectors
```{r}


```



# PCA Analysis on the fastText Tweet Vectors

### Train PCA Model
```{r}
# select the average vector column and perform a PCA
train.pr <- train %>%
  select(avg_vec_50) %>% 
  unnest(avg_vec_50) %>% 
  as.matrix() %>% 
  prcomp()
```

### Evaluate the PCA Model with 3D Graph
```{r}
# create a dataframe with the pca output to examine the first 3 components

pca_evaluation <- as_data_frame(train.pr$x)
pca_evaluation$label <- factor(train$complaint_label)
pca_evaluation$tweet_text <- train$tweet_text

# evalute the first three components with a 3d plot

plot_ly(pca_evaluation, x = ~PC1, y = ~PC2, z = ~PC3, color = ~label, colors = c('#BF382A', '#0C4B8E'),
        hoverinfo = 'text', text = ~tweet_text) %>% 
  add_markers() %>% 
  layout(scene = list(xaxis = list(title = 'PC1'),
                      yaxis = list(title = 'PC2'),
                      zaxis = list(title = 'PC3')))

# looks good to me. The complaints seperate well.
```

### Scree Plot of Principal Components
```{r}
# make a scree plot of the components
screeplot(train.pr, type = "line", npcs = 20)

# the first component must be the amount of information about a word learned by the fasttext model
# let's go with 15 components with this model.


```


# Project Word Vectors onto PCA
Here, we run the VectorizeTweet (PCA class) function on each of the tweet vectors. The VectorizeTweet PCA output summarizes the tweet vector in four principal components.
```{r}
train <- train %>% 
  mutate(avg_vec_50_PCA = map(avg_vec_50, VectorizeTweet, model = train.pr, component = "PC4"))
```


```{r}

save(train.pr, file = "models/fullClassifierOne/prcomp4.dat")

save(train, file = "models/fullClassifierOne/train_data.dat")

rm(list = ls())

load("models/fullClassifierOne/train_data.dat")

trainIndex <- createDataPartition(train$complaint_label, p = .8, 
                                  list = FALSE, 
                                  times = 1)


test_data <- train[-trainIndex,]
test_labels <- test_data %>% select(complaint_label) %>% transmute(label = complaint_label)
train_data <- train[trainIndex,]
train_labels <- train_data %>% select(complaint_label) %>% transmute(label = complaint_label)
train_data_vec <- train_data %>%  select(tweetFeatures, avg_vec_50, avg_vec_50_PCA) %>% unnest %>% mutate_all(as.numeric)
test_data_vec <- test_data %>%  select(tweetFeatures, avg_vec_50, avg_vec_50_PCA) %>% unnest %>% mutate_all(as.numeric)

dtrain <- xgb.DMatrix(sparse.model.matrix(~.-1, train_data_vec), label = as.numeric(train_labels$label))
dtest <- xgb.DMatrix(sparse.model.matrix(~.-1, test_data_vec), label = as.numeric(test_labels$label))
dtest1 <- xgb.DMatrix(sparse.model.matrix(~.-1, test_data_vec))
watchlist <- list(train = dtrain, eval = dtest)


param <- list(max_depth = 2, eta = 1, silent = 1, nthread = 2, 
              objective = logregobj, eval_metric = evalerror)

bst <- xgboost(data = sparse.model.matrix(~.-1, train_data_vec), label = as.numeric(train_labels$label), max.depth = 4,
               eta = 0.2, nthread = 2, nrounds = 100, objective = logregobj, eval_metric = evalerror)

test_data$prediction <- predict(bst, dtest1)

predict(bst, dtest1) %>% map_chr(function(x) ifelse(x > 0, "complaint", "non-complaint"))
  

test_data %<>%
  mutate(predict_label = case_when(prediction > 0 ~ "Complaint",
                                   TRUE ~ "Non-complaint"),
         complaint_label = case_when(complaint_label == 1 ~ "Complaint",
                                   TRUE ~ "Non-complaint")) %>% 
  mutate(accuracy = predict_label == complaint_label)

confusionMatrix(factor(test_data$complaint_label), factor(test_data$predict_label))

save(bst, file = "models/fullClassifierOne/xgb.modelPC4.dat")
xgb.save(bst, 'models/fullClassifierOne/xgb.modelPC4-xgb_version.dat')
save(test_data, file = "models/fullClassifierOne/test_data.dat")


```


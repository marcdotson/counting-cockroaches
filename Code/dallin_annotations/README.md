README
================

Dallin Annotations
==================

In this folder, I will keep all of my .rmd documents. Some of them will be created by me, but most will take Adriel's code in other folders and annotate it. I hope that by doing so, I will better understand his code, and be able to contribute more to writing the paper on our reasearch. Additionally, I hope these annotations help future RA's understand the code as well.

The numbering of the files is an attempt to be consistent with the tidyverse style guide section 1.1 (found at <https://style.tidyverse.org/>). As I work more with the code, I will do my best to ensure that the numbering reflects a logical flow for the code.

If the documents only contain a single, massive block of code, this means that I haven't gotten around to annotating that script.

Below, I have a quick and brief documentation of what I understand about each file.

Overview of Files
=================

Note: Files prefaced by 99\_ are files that I don't understand completely, and seem to be more related to data wrangling than model building.

### 01\_test\_rds\_connection.R

This file contains the credentials necessary to log into the R Data Server. It also has brief code for extracting some of the tweets from the database, and it writes the data to a .txt file.

### 02\_visualize\_twitter\_data.R

This file takes airline twitter data and creates a few visualizations (i.e. wordclouds) on the tweet text. It was formerly called CountingCockroaches.R.

### 03\_analyze\_complaint\_categories.Rmd

This file analyzes our train/test dataset, documents the label types, visualizes the number of complaints to non-complaints, and visualizes the number of each complaint category. Additionally, it runs a tf\_idf analysis on the catch-all complaint category to see if another category can be identified.

### 04\_fasttext\_supervised\_classifier.Rmd

This file documents the building of our fastText supervised classifier model.

### 05\_define\_culstering\_functions.R

These functions will be referenced in future scripts to help build our model. One especially important function here is the avg\_word\_vec function.

##### Average Word Vector Function

This function will take the word vectors for each word in a tweet, and will average the vectors to create a single vector representing the tweet as a whole. The word vectors will be generated by a fasttext unsupervised model. This will effectively become a mathmatical summary of each tweet, and will be a valuable input to our model.

### 06\_define\_prediction\_functions.R

This file contains a lot of useful functions for our model. Two important functions defined here are the VectorizeTweet() and TweetFeatures() functions.

##### VectorizeTweet Function.

This function has two classes: one for use with a unsupervised fastText model, and another for use with a PCA model trained on the output of the VectorizeTweet() function. This class is functionally identical to the avg\_word\_vec() function (as far as I understand--looking at the code they should do the exact same thing, so I'm not entirely sure why we have two nearly identical functions).

The first class (used with an unsupervised fastText model) breaks a tweet down into individual words, and finds the word vectors associated with each of these words. After these word vectors are stored in a dataframe, each variable of the dataframe is averaged to produce a single vector (the same size as an individual word vector). This "tweet vector" can be thought of as a mathematical summary of the tweet.

The second class (used with a PCA model trained on the output of the first class), produces a tibble with the first n principal components of the tweet vector (where n is defined by the user).

##### TweetFeatures Function

This function creates a simple summary of a tweet's features (i.e. the number of exclamation points, the number of links used, and the number of times this user has been mentioned previously in the data).

### 07\_construct\_classifier.Rmd

This document contains the code that builds models referenced in other files, namely the model50-tweet\_sample\_2M\_noRT.dat and xgb.modelPC4-xgb\_version.dat. The xgb.modelPC4-xgb\_version.dat model is the full, frankenstein model we have built. When I reference "the model," or the "frankenstein model," this is the model I am referring to.

### 08\_define\_tweet\_complaint\_predictor\_function.R

tweet\_complaint\_predictor.Rmd builds a function named newTweetComplaintPredictor, which is referenced in serveral other scripts. This function uses our model to predict

### 09\_classifier\_testing.Rmd

This file references the full model, and makes predictions on single tweets. You can use it to see how the model will react with tweets you find online, as well as tweets you write yourself.

Note: I can't seem to find the logit\_classifier\_func.R file anywhere. It's referenced in classifier\_construction.R

### 99\_batch\_classification.Rmd

### 99\_build\_fasttext\_classification\_model.Rmd

In this file, I create my own fastText supervised classification model to see how well it can classify on its own (i.e. without the help of a PCA and XGBoost model). This will be a good base to compare against our own xgboost model built in 07\_construct\_classifier.Rmd

Note: Code still has a few places it needs to be tweaked and isn't fully funcitonal yet--I need to write the .txt files and figure out how to make predictions with the trained model.

### 99\_fasttext\_clust\_EDA.Rmd

### 99\_json\_parse\_upload.Rmd

### 99\_logit\_regression\_classifier.Rmd

### 99\_read\_tweets\_json.Rmd

### 99\_tweet\_object.Rmd
